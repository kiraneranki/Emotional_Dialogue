{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotional-Dialogue.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORTpLNmaSgRBCUfTq9fwBN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiraneranki/Emotional_Dialogue/blob/master/emotional_Dialogue.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpgrmDdp5ygp"
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from rl_model.data_reader import Data_Reader\n",
        "import rl_model.data_parser\n",
        "import config\n",
        "\n",
        "from rl_model.model import Seq2Seq_chatbot\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSlVHPsJ6dt3"
      },
      "source": [
        "Set Global Paramters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2NZBoyr6cpO"
      },
      "source": [
        "### Global Parameters ###\n",
        "checkpoint = config.CHECKPOINT\n",
        "model_path = config.train_model_path\n",
        "model_name = config.train_model_name\n",
        "start_epoch = config.start_epoch\n",
        "\n",
        "word_count_threshold = config.WC_threshold\n",
        "\n",
        "### Train Parameters ###\n",
        "dim_wordvec = 300\n",
        "dim_hidden = 1000\n",
        "\n",
        "n_encode_lstm_step = 22 + 22\n",
        "n_decode_lstm_step = 22\n",
        "\n",
        "epochs = 500  \n",
        "batch_size = 100\n",
        "learning_rate = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q19SJ_fJ7kG3"
      },
      "source": [
        "Training Functions of the Model ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Trhck9vQ6wce"
      },
      "source": [
        "# Training functions \n",
        "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
        "    if not hasattr(sequences, '__len__'):\n",
        "        raise ValueError('`sequences` must be iterable.')\n",
        "    lengths = []\n",
        "    for x in sequences:\n",
        "        if not hasattr(x, '__len__'):\n",
        "            raise ValueError('`sequences` must be a list of iterables. '\n",
        "                             'Found non-iterable: ' + str(x))\n",
        "        lengths.append(len(x))\n",
        "\n",
        "    num_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    # take the sample shape from the first non empty sequence\n",
        "    # checking for consistency in the main loop below.\n",
        "    sample_shape = tuple()\n",
        "    for s in sequences:\n",
        "        if len(s) > 0:\n",
        "            sample_shape = np.asarray(s).shape[1:]\n",
        "            break\n",
        "\n",
        "    x = (np.ones((num_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if not len(s):\n",
        "            continue  # empty list/array was found\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
        "\n",
        "        # check `trunc` has expected shape\n",
        "        trunc = np.asarray(trunc, dtype=dtype)\n",
        "        if trunc.shape[1:] != sample_shape:\n",
        "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
        "                             (trunc.shape[1:], idx, sample_shape))\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
        "    return x\n",
        "\n",
        "def train():\n",
        "    wordtoix, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
        "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
        "\n",
        "    model = Seq2Seq_chatbot(\n",
        "            dim_wordvec=dim_wordvec,\n",
        "            n_words=len(wordtoix),\n",
        "            dim_hidden=dim_hidden,\n",
        "            batch_size=batch_size,\n",
        "            n_encode_lstm_step=n_encode_lstm_step,\n",
        "            n_decode_lstm_step=n_decode_lstm_step,\n",
        "            bias_init_vector=bias_init_vector,\n",
        "            lr=learning_rate)\n",
        "\n",
        "    train_op, tf_loss, word_vectors, tf_caption, tf_caption_mask, inter_value = model.build_model()\n",
        "\n",
        "    saver = tf.train.Saver(max_to_keep=100)\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "    \n",
        "    if checkpoint:\n",
        "        print(\"Use Model {}.\".format(model_name))\n",
        "        saver.restore(sess, os.path.join(model_path, model_name))\n",
        "        print(\"Model {} restored.\".format(model_name))\n",
        "    else:\n",
        "        print(\"Restart training...\")\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "    dr = Data_Reader()\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        n_batch = dr.get_batch_num(batch_size)\n",
        "        for batch in range(n_batch):\n",
        "            start_time = time.time()\n",
        "\n",
        "            batch_X, batch_Y = dr.generate_training_batch(batch_size)\n",
        "\n",
        "            for i in range(len(batch_X)):\n",
        "                batch_X[i] = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in batch_X[i]]\n",
        "                # batch_X[i].insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n",
        "                if len(batch_X[i]) > n_encode_lstm_step:\n",
        "                    batch_X[i] = batch_X[i][:n_encode_lstm_step]\n",
        "                else:\n",
        "                    for _ in range(len(batch_X[i]), n_encode_lstm_step):\n",
        "                        batch_X[i].append(np.zeros(dim_wordvec))\n",
        "\n",
        "            current_feats = np.array(batch_X)\n",
        "\n",
        "            current_captions = batch_Y\n",
        "            current_captions = map(lambda x: '<bos> ' + x, current_captions)\n",
        "            current_captions = map(lambda x: x.replace('.', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace(',', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('\"', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('\\n', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('?', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('!', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('\\\\', ''), current_captions)\n",
        "            current_captions = map(lambda x: x.replace('/', ''), current_captions)\n",
        "\n",
        "            for idx, each_cap in enumerate(current_captions):\n",
        "                word = each_cap.lower().split(' ')\n",
        "                if len(word) < n_decode_lstm_step:\n",
        "                    current_captions[idx] = current_captions[idx] + ' <eos>'\n",
        "                else:\n",
        "                    new_word = ''\n",
        "                    for i in range(n_decode_lstm_step-1):\n",
        "                        new_word = new_word + word[i] + ' '\n",
        "                    current_captions[idx] = new_word + '<eos>'\n",
        "\n",
        "            current_caption_ind = []\n",
        "            for cap in current_captions:\n",
        "                current_word_ind = []\n",
        "                for word in cap.lower().split(' '):\n",
        "                    if word in wordtoix:\n",
        "                        current_word_ind.append(wordtoix[word])\n",
        "                    else:\n",
        "                        current_word_ind.append(wordtoix['<unk>'])\n",
        "                current_caption_ind.append(current_word_ind)\n",
        "\n",
        "            current_caption_matrix = pad_sequences(current_caption_ind, padding='post', maxlen=n_decode_lstm_step)\n",
        "            current_caption_matrix = np.hstack([current_caption_matrix, np.zeros([len(current_caption_matrix), 1])]).astype(int)\n",
        "            current_caption_masks = np.zeros((current_caption_matrix.shape[0], current_caption_matrix.shape[1]))\n",
        "            nonzeros = np.array(map(lambda x: (x != 0).sum() + 1, current_caption_matrix))\n",
        "\n",
        "            for ind, row in enumerate(current_caption_masks):\n",
        "                row[:nonzeros[ind]] = 1\n",
        "\n",
        "            if batch % 100 == 0:\n",
        "                _, loss_val = sess.run(\n",
        "                        [train_op, tf_loss],\n",
        "                        feed_dict={\n",
        "                            word_vectors: current_feats,\n",
        "                            tf_caption: current_caption_matrix,\n",
        "                            tf_caption_mask: current_caption_masks\n",
        "                        })\n",
        "                print(\"Epoch: {}, batch: {}, loss: {}, Elapsed time: {}\".format(epoch, batch, loss_val, time.time() - start_time))\n",
        "            else:\n",
        "                _ = sess.run(train_op,\n",
        "                             feed_dict={\n",
        "                                word_vectors: current_feats,\n",
        "                                tf_caption: current_caption_matrix,\n",
        "                                tf_caption_mask: current_caption_masks\n",
        "                            })\n",
        "\n",
        "\n",
        "        print(\"Epoch \", epoch, \" is done. Saving the model ...\")\n",
        "        saver.save(sess, os.path.join(model_path, 'model'), global_step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgtXR0b-7etM"
      },
      "source": [
        "Test Parameters ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4C3dVKI7boo"
      },
      "source": [
        "=====================================================\n",
        "# Global Parameters\n",
        "#=====================================================\n",
        "default_model_path = './model/Seq2Seq/model-77'\n",
        "testing_data_path = 'sample_input.txt' if len(sys.argv) <= 2 else sys.argv[2]\n",
        "output_path = 'sample_output_S2S.txt' if len(sys.argv) <= 3 else sys.argv[3]\n",
        "\n",
        "word_count_threshold = config.WC_threshold\n",
        "\n",
        "#=====================================================\n",
        "# Train Parameters\n",
        "#=====================================================\n",
        "dim_wordvec = 300\n",
        "dim_hidden = 1000\n",
        "\n",
        "n_encode_lstm_step = 22 + 1 # one random normal as the first timestep\n",
        "n_decode_lstm_step = 22\n",
        "\n",
        "batch_size = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hkpg37c7sme"
      },
      "source": [
        "# Testing functions\n",
        "\"\"\" Extract only the vocabulary part of the data \"\"\"\n",
        "def refine(data):\n",
        "    words = re.findall(\"[a-zA-Z'-]+\", data)\n",
        "    words = [\"\".join(word.split(\"'\")) for word in words]\n",
        "    # words = [\"\".join(word.split(\"-\")) for word in words]\n",
        "    data = ' '.join(words)\n",
        "    return data\n",
        "\n",
        "def test(model_path=default_model_path):\n",
        "    testing_data = open(testing_data_path, 'r').read().split('\\n')\n",
        "\n",
        "    word_vector = KeyedVectors.load_word2vec_format('model/word_vector.bin', binary=True)\n",
        "\n",
        "    _, ixtoword, bias_init_vector = data_parser.preProBuildWordVocab(word_count_threshold=word_count_threshold)\n",
        "\n",
        "    model = Seq2Seq_chatbot(\n",
        "            dim_wordvec=dim_wordvec,\n",
        "            n_words=len(ixtoword),\n",
        "            dim_hidden=dim_hidden,\n",
        "            batch_size=batch_size,\n",
        "            n_encode_lstm_step=n_encode_lstm_step,\n",
        "            n_decode_lstm_step=n_decode_lstm_step,\n",
        "            bias_init_vector=bias_init_vector)\n",
        "\n",
        "    word_vectors, caption_tf, probs, _ = model.build_generator()\n",
        "\n",
        "    sess = tf.InteractiveSession()\n",
        "\n",
        "    saver = tf.train.Saver()\n",
        "    try:\n",
        "        print('\\n=== Use model', model_path, '===\\n')\n",
        "        saver.restore(sess, model_path)\n",
        "    except:\n",
        "        print('\\nUse default model\\n')\n",
        "        saver.restore(sess, default_model_path)\n",
        "\n",
        "    with open(output_path, 'w') as out:\n",
        "        generated_sentences = []\n",
        "        bleu_score_avg = [0., 0.]\n",
        "        for idx, question in enumerate(testing_data):\n",
        "            print('question =>', question)\n",
        "\n",
        "            question = [refine(w) for w in question.lower().split()]\n",
        "            question = [word_vector[w] if w in word_vector else np.zeros(dim_wordvec) for w in question]\n",
        "            question.insert(0, np.random.normal(size=(dim_wordvec,))) # insert random normal at the first step\n",
        "\n",
        "            if len(question) > n_encode_lstm_step:\n",
        "                question = question[:n_encode_lstm_step]\n",
        "            else:\n",
        "                for _ in range(len(question), n_encode_lstm_step):\n",
        "                    question.append(np.zeros(dim_wordvec))\n",
        "\n",
        "            question = np.array([question]) # 1x22x300\n",
        "    \n",
        "            generated_word_index, prob_logit = sess.run([caption_tf, probs], feed_dict={word_vectors: question})\n",
        "            \n",
        "            # remove <unk> to second high prob. word\n",
        "            for i in range(len(generated_word_index)):\n",
        "                if generated_word_index[i] == 3:\n",
        "                    sort_prob_logit = sorted(prob_logit[i][0])\n",
        "                    maxindex = np.where(prob_logit[i][0] == sort_prob_logit[-1])[0][0]\n",
        "                    secmaxindex = np.where(prob_logit[i][0] == sort_prob_logit[-2])[0][0]\n",
        "                    generated_word_index[i] = secmaxindex\n",
        "\n",
        "            generated_words = []\n",
        "            for ind in generated_word_index:\n",
        "                generated_words.append(ixtoword[ind])\n",
        "\n",
        "            # generate sentence\n",
        "            punctuation = np.argmax(np.array(generated_words) == '<eos>') + 1\n",
        "            generated_words = generated_words[:punctuation]\n",
        "            generated_sentence = ' '.join(generated_words)\n",
        "\n",
        "            # modify the output sentence \n",
        "            generated_sentence = generated_sentence.replace('<bos> ', '')\n",
        "            generated_sentence = generated_sentence.replace(' <eos>', '')\n",
        "            generated_sentence = generated_sentence.replace('--', '')\n",
        "            generated_sentence = generated_sentence.split('  ')\n",
        "            for i in range(len(generated_sentence)):\n",
        "                generated_sentence[i] = generated_sentence[i].strip()\n",
        "                if len(generated_sentence[i]) > 1:\n",
        "                    generated_sentence[i] = generated_sentence[i][0].upper() + generated_sentence[i][1:] + '.'\n",
        "                else:\n",
        "                    generated_sentence[i] = generated_sentence[i].upper()\n",
        "            generated_sentence = ' '.join(generated_sentence)\n",
        "            generated_sentence = generated_sentence.replace(' i ', ' I ')\n",
        "            generated_sentence = generated_sentence.replace(\"i'm\", \"I'm\")\n",
        "            generated_sentence = generated_sentence.replace(\"i'd\", \"I'd\")\n",
        "            generated_sentence = generated_sentence.replace(\"i'll\", \"I'll\")\n",
        "            generated_sentence = generated_sentence.replace(\"i'v\", \"I'v\")\n",
        "            generated_sentence = generated_sentence.replace(\" - \", \"\")\n",
        "\n",
        "            print('generated_sentence =>', generated_sentence)\n",
        "            out.write(generated_sentence + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}